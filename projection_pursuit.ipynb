{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6179ece8-975f-440c-b927-b7bbc7b94e15",
   "metadata": {},
   "source": [
    "## PP-ANN Vs. ANNOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a55d715b-7f67-4981-a11a-34e1d875cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skpp import ProjectionPursuitRegressor\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58434060-6801-4778-8efe-974066323a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionPursuitANN:\n",
    "    def __init__(self, r=3, stage_maxiter=50, backfit_maxiter=5, tol=1e-4):\n",
    "        self.r = r\n",
    "        self.stage_maxiter = stage_maxiter\n",
    "        self.backfit_maxiter = backfit_maxiter\n",
    "        self.tol = tol\n",
    "        self.ppr = None\n",
    "        self.knn = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, k=10):\n",
    "        try:\n",
    "            # Scale the data\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            \n",
    "            # Use a small number of random reference points\n",
    "            n_samples = X_scaled.shape[0]\n",
    "            max_targets = min(5, n_samples // 20)\n",
    "            reference_indices = np.random.choice(n_samples, max_targets, replace=False)\n",
    "            target = np.zeros((n_samples, max_targets))\n",
    "            for i, ref_idx in enumerate(reference_indices):\n",
    "                target[:, i] = np.linalg.norm(X_scaled - X_scaled[ref_idx].reshape(1, -1), axis=1)\n",
    "            \n",
    "            # Fit projection pursuit regressor with conservative parameters\n",
    "            self.ppr = ProjectionPursuitRegressor(\n",
    "                r=self.r,\n",
    "                stage_maxiter=self.stage_maxiter,\n",
    "                backfit_maxiter=self.backfit_maxiter,\n",
    "                eps_stage=self.tol,\n",
    "                opt_level='medium',\n",
    "                random_state=42\n",
    "            )\n",
    "            self.ppr.fit(X_scaled, target)\n",
    "            \n",
    "            # Project the data\n",
    "            X_projected = self.ppr.transform(X_scaled)\n",
    "            \n",
    "            # Build kNN model on projected data\n",
    "            self.knn = NearestNeighbors(n_neighbors=k, algorithm='brute')\n",
    "            self.knn.fit(X_projected)\n",
    "            \n",
    "            return self\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting PP-ANN: {e}\")\n",
    "            # Fallback to standard kNN\n",
    "            self.knn = NearestNeighbors(n_neighbors=k)\n",
    "            self.knn.fit(X)\n",
    "            return self\n",
    "    \n",
    "    def kneighbors(self, X, n_neighbors=None, return_distance=True):\n",
    "        try:\n",
    "            if self.ppr is not None:\n",
    "                X_scaled = self.scaler.transform(X)\n",
    "                X_projected = self.ppr.transform(X_scaled)\n",
    "                return self.knn.kneighbors(X_projected, n_neighbors, return_distance)\n",
    "            else:\n",
    "                return self.knn.kneighbors(X, n_neighbors, return_distance)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in kneighbors: {e}\")\n",
    "            indices = np.random.randint(0, self.knn._fit_X.shape[0], \n",
    "                                          size=(X.shape[0], n_neighbors or self.knn.n_neighbors))\n",
    "            if return_distance:\n",
    "                distances = np.ones_like(indices, dtype=float)\n",
    "                return distances, indices\n",
    "            return indices\n",
    "\n",
    "# ANNOY wrapper for a consistent API\n",
    "class ANNOYWrapper:\n",
    "    def __init__(self, n_trees=10, metric='angular'):\n",
    "        self.n_trees = n_trees\n",
    "        self.metric = metric\n",
    "        self.index = None\n",
    "        self.dim = None\n",
    "        self.data = None\n",
    "        \n",
    "    def fit(self, X, k=10):\n",
    "        self.data = X\n",
    "        self.dim = X.shape[1]\n",
    "        \n",
    "        self.index = AnnoyIndex(self.dim, self.metric)\n",
    "        for i, x in enumerate(X):\n",
    "            self.index.add_item(i, x)\n",
    "            \n",
    "        self.index.build(self.n_trees)\n",
    "        return self\n",
    "    \n",
    "    def kneighbors(self, X, n_neighbors=5, return_distance=True):\n",
    "        indices = []\n",
    "        distances = []\n",
    "        for x in X:\n",
    "            idx, dist = self.index.get_nns_by_vector(x, n_neighbors, include_distances=True)\n",
    "            indices.append(idx)\n",
    "            distances.append(dist)\n",
    "        if return_distance:\n",
    "            return np.array(distances), np.array(indices)\n",
    "        else:\n",
    "            return np.array(indices)\n",
    "\n",
    "# Load GloVe word embeddings dataset\n",
    "def load_glove_embeddings(file_path='glove.6B.50d.txt', limit=10000):\n",
    "    print(f\"Loading GloVe embeddings from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "        print(f\"Downloading GloVe embeddings from {url}...\")\n",
    "        urllib.request.urlretrieve(url, \"glove.6B.zip\")\n",
    "        with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        print(\"Download complete\")\n",
    "    \n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            word_to_index[word] = i\n",
    "            embeddings.append(vector)\n",
    "    return np.array(embeddings), word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cc28c31-1e1f-4da5-86d2-96aacaeb8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from glove.6B.50d.txt...\n",
      "Loaded 10000 word embeddings with 50 dimensions\n",
      "Building indices for all methods...\n",
      "  Building index for PP-ANN (r=15)...\n",
      "  Building index for PP-ANN (r=30)...\n",
      "  Building index for ANNOY (10 trees)...\n",
      "  Building index for ANNOY (50 trees)...\n",
      "  Building index for ANNOY (100 trees)...\n",
      "  Building index for Exact NN...\n",
      "\n",
      "Benchmarking query performance for PP-ANN (r=15)...\n",
      "\n",
      "Benchmarking query performance for PP-ANN (r=30)...\n",
      "\n",
      "Benchmarking query performance for ANNOY (10 trees)...\n",
      "\n",
      "Benchmarking query performance for ANNOY (50 trees)...\n",
      "\n",
      "Benchmarking query performance for ANNOY (100 trees)...\n",
      "\n",
      "Benchmarking query performance for Exact NN...\n",
      "\n",
      "Benchmark Results:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Method               Fit Time (s) Query Time (ms)  Recall@10  Semantic Acc\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PP-ANN (r=15)        28.59        0.03             0.3359     0.2222      \n",
      "PP-ANN (r=30)        124.68       0.03             0.5314     0.5556      \n",
      "ANNOY (10 trees)     0.07         0.02             0.6083     0.4444      \n",
      "ANNOY (50 trees)     0.18         0.08             0.7595     0.6667      \n",
      "ANNOY (100 trees)    0.34         0.13             0.7748     0.6667      \n",
      "Exact NN             0.00         0.04             1.0000     0.6667      \n",
      "\n",
      "Semantic Search Examples:\n",
      "====================================================================================================\n",
      "\n",
      "PP-ANN (r=15):\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✗\n",
      "Neighbors: king, commercials, hbo, bros., kennedy\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✗\n",
      "Neighbors: man, when, another, knew, took\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✗\n",
      "Neighbors: france, french, congo, congolese, eritrea\n",
      "\n",
      "\n",
      "PP-ANN (r=30):\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✗\n",
      "Neighbors: king, iii, iv, prince, lord\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✓\n",
      "Neighbors: man, woman, blind, another, one\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✓\n",
      "Neighbors: france, french, paris, brussels, belgium\n",
      "\n",
      "\n",
      "ANNOY (10 trees):\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✗\n",
      "Neighbors: king, prince, ruler, monarch, vi\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✓\n",
      "Neighbors: man, woman, boy, old, her\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✓\n",
      "Neighbors: france, french, belgium, paris, netherlands\n",
      "\n",
      "\n",
      "ANNOY (50 trees):\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✓\n",
      "Neighbors: king, prince, queen, emperor, throne\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✓\n",
      "Neighbors: man, woman, boy, old, him\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✓\n",
      "Neighbors: france, french, belgium, paris, spain\n",
      "\n",
      "\n",
      "ANNOY (100 trees):\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✓\n",
      "Neighbors: king, prince, queen, ii, emperor\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✓\n",
      "Neighbors: man, woman, boy, another, old\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✓\n",
      "Neighbors: france, french, belgium, paris, spain\n",
      "\n",
      "\n",
      "Exact NN:\n",
      "--------------------------------------------------\n",
      "Query: 'king', Expected: 'queen', Found: ✓\n",
      "Neighbors: king, prince, queen, uncle, ii\n",
      "\n",
      "Query: 'man', Expected: 'woman', Found: ✓\n",
      "Neighbors: man, woman, another, boy, one\n",
      "\n",
      "Query: 'france', Expected: 'paris', Found: ✓\n",
      "Neighbors: france, french, belgium, paris, netherlands\n",
      "\n",
      "\n",
      "Overall Benchmark Summary:\n",
      "Average Recall@10 across all methods: 0.6683\n",
      "Average Semantic Accuracy across all methods: 0.5370\n",
      "Total embeddings benchmarked: 10000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_similarity_search(embeddings, word_to_index, index_to_word, method, n_queries=10, k=5):\n",
    "    semantic_pairs = [\n",
    "        ('king', 'queen'),\n",
    "        ('man', 'woman'),\n",
    "        ('france', 'paris'),\n",
    "        ('japan', 'tokyo'),\n",
    "        ('computer', 'laptop'),\n",
    "        ('car', 'vehicle'),\n",
    "        ('happy', 'sad'),\n",
    "        ('river', 'lake'),\n",
    "        ('sun', 'moon'),\n",
    "        ('doctor', 'hospital')\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    valid_pairs = [(w1, w2) for w1, w2 in semantic_pairs \n",
    "                   if w1 in word_to_index and w2 in word_to_index][:n_queries]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        print(\"No valid semantic pairs found in the vocabulary!\")\n",
    "        return {}\n",
    "    \n",
    "    for w1, w2 in valid_pairs:\n",
    "        idx1 = word_to_index[w1]\n",
    "        query_vector = embeddings[idx1].reshape(1, -1)\n",
    "        \n",
    "        if method.__class__.__name__ == 'ANNOYWrapper':\n",
    "            _, neighbor_indices = method.kneighbors(query_vector, n_neighbors=k+1)\n",
    "            neighbor_indices = neighbor_indices[0]\n",
    "        else:\n",
    "            _, neighbor_indices = method.kneighbors(query_vector, n_neighbors=k+1)\n",
    "            neighbor_indices = neighbor_indices[0]\n",
    "        \n",
    "        neighbors = [index_to_word[idx] for idx in neighbor_indices if idx != idx1]\n",
    "        rank = -1\n",
    "        if w2 in [index_to_word[idx] for idx in neighbor_indices]:\n",
    "            for i, idx in enumerate(neighbor_indices):\n",
    "                if index_to_word[idx] == w2:\n",
    "                    rank = i\n",
    "                    break\n",
    "        \n",
    "        neighbor_words = [index_to_word[idx] for idx in neighbor_indices[:k]]\n",
    "        results[w1] = {\n",
    "            'query': w1,\n",
    "            'expected': w2,\n",
    "            'neighbors': neighbor_words,\n",
    "            'found': w2 in neighbor_words,\n",
    "            'rank': rank\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark function with adjustable query count (without memory measurements)\n",
    "def benchmark_ann_methods(embeddings, word_to_index, random_seed=42, n_queries=1000):\n",
    "    np.random.seed(random_seed)\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "    \n",
    "    n_samples = embeddings.shape[0]\n",
    "    X_train = embeddings\n",
    "    k = 10  # Number of neighbors\n",
    "    \n",
    "    results = {}\n",
    "    semantic_results = {}\n",
    "    \n",
    "    methods = {\n",
    "        'PP-ANN (r=15)': ProjectionPursuitANN(r=15, stage_maxiter=500, backfit_maxiter=5),\n",
    "        'PP-ANN (r=30)': ProjectionPursuitANN(r=30, stage_maxiter=500, backfit_maxiter=5),\n",
    "        'ANNOY (10 trees)': ANNOYWrapper(n_trees=10, metric='angular'),\n",
    "        'ANNOY (50 trees)': ANNOYWrapper(n_trees=50, metric='angular'),\n",
    "        'ANNOY (100 trees)': ANNOYWrapper(n_trees=100, metric='angular'),\n",
    "        'Exact NN': NearestNeighbors(n_neighbors=k, algorithm='brute')\n",
    "    }\n",
    "    \n",
    "    query_indices = np.random.choice(n_samples, n_queries, replace=False)\n",
    "    X_query = X_train[query_indices]\n",
    "    \n",
    "    print(\"Building indices for all methods...\")\n",
    "    for name, method in methods.items():\n",
    "        print(f\"  Building index for {name}...\")\n",
    "        method.fit(X_train, k)\n",
    "    \n",
    "    for name, method in methods.items():\n",
    "        print(f\"\\nBenchmarking query performance for {name}...\")\n",
    "        try:\n",
    "            # Measure fit time\n",
    "            t0 = time.time()\n",
    "            method.fit(X_train, k)\n",
    "            fit_time = time.time() - t0\n",
    "            \n",
    "            # Measure query performance\n",
    "            chunk_size = 200\n",
    "            all_distances = []\n",
    "            all_indices = []\n",
    "            query_start = time.time()\n",
    "            for i in range(0, n_queries, chunk_size):\n",
    "                end_idx = min(i + chunk_size, n_queries)\n",
    "                chunk = X_query[i:end_idx]\n",
    "                distances, indices = method.kneighbors(chunk, n_neighbors=k)\n",
    "                all_distances.append(distances)\n",
    "                all_indices.append(indices)\n",
    "            query_time = time.time() - query_start\n",
    "            distances = np.vstack(all_distances)\n",
    "            indices = np.vstack(all_indices)\n",
    "            avg_query_time = query_time / n_queries\n",
    "            \n",
    "            # Compute recall for non-exact methods\n",
    "            if name != 'Exact NN':\n",
    "                nn = NearestNeighbors(n_neighbors=k, algorithm='brute')\n",
    "                nn.fit(X_train)\n",
    "                recall_sum = 0\n",
    "                for i in range(0, n_queries, chunk_size):\n",
    "                    end_idx = min(i + chunk_size, n_queries)\n",
    "                    chunk = X_query[i:end_idx]\n",
    "                    _, exact_indices = nn.kneighbors(chunk, n_neighbors=k)\n",
    "                    for j in range(end_idx - i):\n",
    "                        method_neighbors = set(indices[i+j])\n",
    "                        exact_neighbors = set(exact_indices[j])\n",
    "                        recall_sum += len(method_neighbors.intersection(exact_neighbors)) / k\n",
    "                recall = recall_sum / n_queries\n",
    "            else:\n",
    "                recall = 1.0\n",
    "            \n",
    "            semantic_eval = evaluate_similarity_search(embeddings, word_to_index, index_to_word, method, n_queries=15)\n",
    "            semantic_accuracy = sum(result['found'] for result in semantic_eval.values()) / len(semantic_eval)\n",
    "            \n",
    "            semantic_results[name] = semantic_eval\n",
    "            results[name] = {\n",
    "                'fit_time': fit_time,\n",
    "                'query_time': avg_query_time,\n",
    "                'recall@k': recall,\n",
    "                'semantic_accuracy': semantic_accuracy\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error benchmarking {name}: {e}\")\n",
    "            results[name] = {\n",
    "                'fit_time': float('nan'),\n",
    "                'query_time': float('nan'),\n",
    "                'recall@k': float('nan'),\n",
    "                'semantic_accuracy': float('nan')\n",
    "            }\n",
    "    \n",
    "    return results, semantic_results\n",
    "\n",
    "# Run the benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    embeddings, word_to_index = load_glove_embeddings(file_path='glove.6B.50d.txt', limit=10000)\n",
    "    print(f\"Loaded {len(embeddings)} word embeddings with {embeddings.shape[1]} dimensions\")\n",
    "    \n",
    "    results, semantic_results = benchmark_ann_methods(embeddings, word_to_index, n_queries=1000)\n",
    "    \n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"-\" * 100)\n",
    "    header = f\"{'Method':<20} {'Fit Time (s)':<12} {'Query Time (ms)':<16} {'Recall@10':<10} {'Semantic Acc':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        query_time_ms = metrics['query_time'] * 1000  # Convert to ms\n",
    "        print(f\"{name:<20} {metrics['fit_time']:<12.2f} {query_time_ms:<16.2f} {metrics['recall@k']:<10.4f} {metrics['semantic_accuracy']:<12.4f}\")\n",
    "    \n",
    "    print(\"\\nSemantic Search Examples:\")\n",
    "    print(\"=\" * 100)\n",
    "    for method_name, method_results in semantic_results.items():\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        print(\"-\" * 50)\n",
    "        for query, result in list(method_results.items())[:3]:\n",
    "            neighbors_str = \", \".join(result['neighbors'])\n",
    "            found_str = \"✓\" if result['found'] else \"✗\"\n",
    "            print(f\"Query: '{query}', Expected: '{result['expected']}', Found: {found_str}\")\n",
    "            print(f\"Neighbors: {neighbors_str}\\n\")\n",
    "    \n",
    "    avg_recall = sum(m['recall@k'] for m in results.values()) / len(results)\n",
    "    avg_semantic = sum(m['semantic_accuracy'] for m in results.values()) / len(results)\n",
    "    print(\"\\nOverall Benchmark Summary:\")\n",
    "    print(f\"Average Recall@10 across all methods: {avg_recall:.4f}\")\n",
    "    print(f\"Average Semantic Accuracy across all methods: {avg_semantic:.4f}\")\n",
    "    print(f\"Total embeddings benchmarked: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6b212-2b1c-4bc7-bd54-e94476ccfc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
